{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルの作成\n",
    "---\n",
    "参考：\n",
    "- [all](http://ni4muraano.hatenablog.com/entry/2017/08/10/101053)\n",
    "- [batch normalization](https://www.kaggle.com/dingdiego/u-net-batchnorm-augmentation-stratification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers.convolutional import Conv2D, ZeroPadding2D, Conv2DTranspose\n",
    "from keras.layers import Input, concatenate, Conv2D, MaxPooling2D, Conv2DTranspose, ZeroPadding2D, ZeroPadding3D, AveragePooling3D\n",
    "from keras.layers import LeakyReLU, BatchNormalization, Activation, Dropout\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "\n",
    "class DenceUNet(object):\n",
    "    def __init__(self, input_channel_count, output_channel_count, first_layer_filter_count):\n",
    "        self.INPUT_IMAGE_SIZE = 512\n",
    "        self.CONCATENATE_AXIS = -1\n",
    "        self.CONV_FILTER_SIZE = 4\n",
    "        self.CONV_STRIDE = 2\n",
    "        self.CONV_PADDING = (1, 1)\n",
    "        self.DECONV_FILTER_SIZE = 2\n",
    "        self.DECONV_STRIDE = 2\n",
    "\n",
    "        # (512 x 512 x input_channel_count)\n",
    "        inputs = Input((self.INPUT_IMAGE_SIZE, self.INPUT_IMAGE_SIZE, input_channel_count))\n",
    "        conv11 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "        conv11 =  BatchNormalization()(conv11)\n",
    "        conc11 = concatenate([inputs, conv11], axis=3)\n",
    "        conv12 = Conv2D(32, (3, 3), activation='relu', padding='same')(conc11)\n",
    "        conv12 =  BatchNormalization()(conv12)\n",
    "        conc12 = concatenate([inputs, conv12], axis=3)\n",
    "        pool1 = MaxPooling2D(pool_size=(2, 2))(conc12)\n",
    "        \n",
    "        conv21 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
    "        conv21 =  BatchNormalization()(conv21)\n",
    "        conc21 = concatenate([pool1, conv21], axis=3)\n",
    "        conv22 = Conv2D(64, (3, 3), activation='relu', padding='same')(conc21)\n",
    "        conv22 =  BatchNormalization()(conv22)\n",
    "        conc22 = concatenate([pool1, conv22], axis=3)\n",
    "        pool2 = MaxPooling2D(pool_size=(2, 2))(conc22)\n",
    "        \n",
    "        conv31 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n",
    "        conv31 =  BatchNormalization()(conv31)\n",
    "        conc31 = concatenate([pool2, conv31], axis=3)\n",
    "        conv32 = Conv2D(128, (3, 3), activation='relu', padding='same')(conc31)\n",
    "        conv32 =  BatchNormalization()(conv32)\n",
    "        conc32 = concatenate([pool2, conv32], axis=3)\n",
    "        pool3 = MaxPooling2D(pool_size=(2, 2))(conc32)\n",
    "        \n",
    "        conv41 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)\n",
    "        conv41 =  BatchNormalization()(conv41)\n",
    "        conc41 = concatenate([pool3, conv41], axis=3)\n",
    "        conv42 = Conv2D(256, (3, 3), activation='relu', padding='same')(conc41)\n",
    "        conv42 =  BatchNormalization()(conv42)\n",
    "        conc42 = concatenate([pool3, conv42], axis=3)\n",
    "        pool4 = MaxPooling2D(pool_size=(2, 2))(conc42)\n",
    "        \n",
    "        conv51 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)\n",
    "        conv51 =  BatchNormalization()(conv51)\n",
    "        conc51 = concatenate([pool4, conv51], axis=3)\n",
    "        conv52 = Conv2D(512, (3, 3), activation='relu', padding='same')(conc51)\n",
    "        conv52 =  BatchNormalization()(conv52)\n",
    "        conc52 = concatenate([pool4, conv52], axis=3)\n",
    "\n",
    "        \n",
    "        up6 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conc52), conc42], axis=3)\n",
    "        conv61 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)\n",
    "        conv61 =  BatchNormalization()(conv61)\n",
    "        conc61 = concatenate([up6, conv61], axis=3)\n",
    "        conv62 = Conv2D(256, (3, 3), activation='relu', padding='same')(conc61)\n",
    "        conv62 =  BatchNormalization()(conv62)\n",
    "        conc62 = concatenate([up6, conv62], axis=3)\n",
    "\n",
    "        up7 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conc62), conv32], axis=3)\n",
    "        conv71 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)\n",
    "        conv71 =  BatchNormalization()(conv71)\n",
    "        conc71 = concatenate([up7, conv71], axis=3)\n",
    "        conv72 = Conv2D(128, (3, 3), activation='relu', padding='same')(conc71)\n",
    "        conv72 =  BatchNormalization()(conv72)\n",
    "        conc72 = concatenate([up7, conv72], axis=3)\n",
    "\n",
    "        up8 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conc72), conv22], axis=3)\n",
    "        conv81 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)\n",
    "        conv81 =  BatchNormalization()(conv81)\n",
    "        conc81 = concatenate([up8, conv81], axis=3)\n",
    "        conv82 = Conv2D(64, (3, 3), activation='relu', padding='same')(conc81)\n",
    "        conv82 =  BatchNormalization()(conv82)\n",
    "        conc82 = concatenate([up8, conv82], axis=3)\n",
    "\n",
    "        up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conc82), conv12], axis=3)\n",
    "        conv91 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)\n",
    "        conv91 =  BatchNormalization()(conv91)\n",
    "        conc91 = concatenate([up9, conv91], axis=3)\n",
    "        conv92 = Conv2D(32, (3, 3), activation='relu', padding='same')(conc91)\n",
    "        conv92 =  BatchNormalization()(conv92)\n",
    "        conc92 = concatenate([up9, conv92], axis=3)\n",
    "\n",
    "        conv10 = Conv2D(1, (1, 1), activation='sigmoid')(conc92)\n",
    "        \n",
    "        self.DenceUNET = Model(inputs=[inputs], outputs=[conv10])\n",
    "        \n",
    "\n",
    "    def _add_encoding_layer(self, filter_count, sequence):\n",
    "        new_sequence = LeakyReLU(0.2)(sequence)\n",
    "        new_sequence = ZeroPadding2D(self.CONV_PADDING)(new_sequence)\n",
    "        new_sequence = Conv2D(filter_count, self.CONV_FILTER_SIZE, strides=self.CONV_STRIDE)(new_sequence)\n",
    "        new_sequence = BatchNormalization()(new_sequence)\n",
    "        return new_sequence\n",
    "\n",
    "    def _add_decoding_layer(self, filter_count, add_drop_layer, sequence):\n",
    "        new_sequence = Activation(activation='relu')(sequence)\n",
    "        new_sequence = Conv2DTranspose(filter_count, self.DECONV_FILTER_SIZE, strides=self.DECONV_STRIDE,\n",
    "                                       kernel_initializer='he_uniform')(new_sequence)\n",
    "        new_sequence = BatchNormalization()(new_sequence)\n",
    "        if add_drop_layer:\n",
    "            new_sequence = Dropout(0.5)(new_sequence)\n",
    "        return new_sequence\n",
    "\n",
    "    def get_model(self):\n",
    "        return self.DenceUNET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### その他関数定義\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "IMAGE_SIZE = 512\n",
    "TRAIN_PERCENTAGE = 0.8\n",
    "\n",
    "# 値を-1から1に正規化する関数\n",
    "def normalize_x(image):\n",
    "    # image = image/127.5 - 1\n",
    "    image = (image-np.min(image)) / (np.max(image)/2) - 1\n",
    "    return image\n",
    "\n",
    "\n",
    "# 値を0から1に正規化する関数\n",
    "def normalize_y(image):\n",
    "    image = image/255\n",
    "    return image\n",
    "\n",
    "\n",
    "# 値を0から255に戻す関数\n",
    "def denormalize_y(image):\n",
    "    image = image*255\n",
    "    return image\n",
    "\n",
    "\n",
    "# インプット画像を読み込む関数\n",
    "def load_X(folder_path, mode='train'):\n",
    "    import glob, SimpleITK\n",
    "    from natsort import natsorted\n",
    "    \n",
    "    mhd_files = natsorted(glob.glob(folder_path+'*.mhd'))\n",
    "    limits = int(len(mhd_files)*TRAIN_PERCENTAGE)\n",
    "\n",
    "    if mode=='test':\n",
    "        images = SimpleITK.GetArrayFromImage(SimpleITK.ReadImage(mhd_files[0]))\n",
    "        \n",
    "        image_files = []\n",
    "        for i in range(images.shape[0]):\n",
    "            image_files.append(mhd_files[limits].split('/')[-1].split('.')[0] +'-'+str(i))    \n",
    "\n",
    "        for mhd_name in mhd_files[(int(limits)+1):]:\n",
    "            mhd_array = SimpleITK.GetArrayFromImage(SimpleITK.ReadImage(mhd_name))\n",
    "            images = np.concatenate([images,mhd_array])\n",
    "            for i in range(mhd_array.shape[0]):\n",
    "                image_files.append(mhd_name.split('/')[-1].split('.')[0] +'-'+str(i))  \n",
    "                \n",
    "    else:\n",
    "        images = SimpleITK.GetArrayFromImage(SimpleITK.ReadImage(mhd_files[0]))\n",
    "        \n",
    "        image_files = []\n",
    "        for i in range(images.shape[0]):\n",
    "            image_files.append(mhd_files[0].split('/')[-1].split('.')[0] +'-'+str(i))    \n",
    "\n",
    "        for mhd_name in mhd_files[1:int(limits)]:\n",
    "            mhd_array = SimpleITK.GetArrayFromImage(SimpleITK.ReadImage(mhd_name))\n",
    "            images = np.concatenate([images,mhd_array])\n",
    "            for i in range(mhd_array.shape[0]):\n",
    "                image_files.append(mhd_name.split('/')[-1].split('.')[0] +'-'+str(i))     \n",
    "    \n",
    "    images = normalize_x(images[:, :, :, np.newaxis])\n",
    "    return images, image_files\n",
    "\n",
    "\n",
    "# ラベル画像を読み込む関数\n",
    "def load_Y(folder_path):\n",
    "    import glob, SimpleITK\n",
    "    from natsort import natsorted\n",
    "    \n",
    "    mhd_files = natsorted(glob.glob(folder_path+'*/*[!2].mhd'))\n",
    "    limits = int(len(mhd_files)*TRAIN_PERCENTAGE)\n",
    "    \n",
    "    images = SimpleITK.GetArrayFromImage(SimpleITK.ReadImage(mhd_files[0]))\n",
    "    for mhd_name in mhd_files[1:int(limits)]:\n",
    "        mhd_array = SimpleITK.GetArrayFromImage(SimpleITK.ReadImage(mhd_name))\n",
    "        images = np.concatenate([images,mhd_array])\n",
    "    \n",
    "    # GTの値の範囲が0-6だったので、1以上は1にしている\n",
    "    images = np.where(images >= 1 , 1 ,images)\n",
    "    return images[:, :, :, np.newaxis]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train/test部分\n",
    "---\n",
    "\n",
    "参考：\n",
    "- [クロスバリデーション](https://lp-tech.net/articles/Y56uo)\n",
    "- [モデルチェックポイント](https://blog.shikoan.com/keras-model-checkpoint-save-best-only/)\n",
    "- [その全て](https://keras.io/ja/callbacks/)\n",
    "\n",
    "- [multi GPU](http://tech.wonderpla.net/entry/2018/01/09/110000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.utils import multi_gpu_model #GPU 2枚用\n",
    "\n",
    "# GPU1つのみの設定\n",
    "if 'tensorflow' == K.backend():\n",
    "    import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.visible_device_list = \"1\"\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.utils import plot_model\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "\n",
    "# ダイス係数を計算\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true = K.flatten(y_true)\n",
    "    y_pred = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true * y_pred)\n",
    "    return 2.0 * intersection / (K.sum(y_true) + K.sum(y_pred) + 1)\n",
    "\n",
    "# ロス関数\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1.0 - dice_coef(y_true, y_pred)\n",
    "\n",
    "\n",
    "# denceU-Netのトレーニングを実行する関数\n",
    "def train_denceunet():\n",
    "    X_train, file_names = load_X('../Preprocess/mhd/', 'train')\n",
    "    Y_train = load_Y('../Preprocess/GroundTruth/')\n",
    "    kf = KFold(n_splits=12, shuffle=False)\n",
    "    \n",
    "    # in:1チャンネル out:1チャンネル\n",
    "    input_channel_count = 1\n",
    "    output_channel_count = 1\n",
    "\n",
    "    # 一番初めのConvolutionフィルタ枚数は64\n",
    "    first_layer_filter_count = 64\n",
    "    BATCH_SIZE = 2\n",
    "    NUM_EPOCH = 30\n",
    "\n",
    "    # denceU-Netの生成\n",
    "    network = DenceUNet(input_channel_count, output_channel_count, first_layer_filter_count)\n",
    "    model = network.get_model()\n",
    "    model.compile(optimizer=Adam(lr=1e-5, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.000000199), metrics=['accuracy', dice_coef], loss=dice_coef_loss )\n",
    "    #model.compile(optimizer=Adam(lr=1e-5, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.000000199), loss='binary_crossentropy', metrics=['accuracy'] )\n",
    "    #model.compile(optimizer=Adam(lr=1e-5), loss=dice_coef_loss, metrics=['accuracy', dice_coef] )\n",
    "    \n",
    "    plot_model(model, to_file='model.png')   \n",
    "    model.summary()\n",
    "    \n",
    "    # GPU 2枚用\n",
    "    # parallel_model = multi_gpu_model(model, gpus=2)\n",
    "    # parallel_model.compile(optimizer=Adam(lr=1e-5, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.000000199), metrics=[dice_coef], loss=dice_coef_loss )\n",
    "    # parallel_model.summary()\n",
    "    \n",
    "    for i, (train_index, eval_index) in enumerate(kf.split(X_train)):\n",
    "        X_tra, X_eval = X_train[train_index], X_train[eval_index]\n",
    "        y_tra, y_eval = Y_train[train_index], Y_train[eval_index]        \n",
    "\n",
    "        cp = ModelCheckpoint(\"weights/weights-\"+str(i)+\"crs-{epoch:02d}ep-{val_loss:.2f}loss.hdf5\",\n",
    "                             monitor=\"val_loss\", verbose=1, save_best_only=True, save_weights_only=False)\n",
    "        learning_history = model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=NUM_EPOCH, callbacks=[cp], validation_data=(X_eval, y_eval))\n",
    "        model.save_weights('weights/weights-ver'+ str(i) +'.hdf5')\n",
    "        \n",
    "        # GPU 2枚用\n",
    "        # learning_history = parallel_model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=NUM_EPOCH, callbacks=[cp], validation_data=(X_eval, y_eval))\n",
    "        # parallel_model.save_weights('weights/weights-ver'+ str(i) +'.hdf5')  # <- ここだけ未確認(model.saveでok?)\n",
    "\n",
    "        \n",
    "        plt.plot(range(1, NUM_EPOCH+1), learning_history.history['dice_coef'], label=\"training\")\n",
    "        plt.plot(range(1, NUM_EPOCH+1), learning_history.history['val_dice_coef'], label=\"validation\")\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        plt.ylim(0,1)\n",
    "        plt.grid(axis='y', color='lightgray')\n",
    "        plt.savefig('progress-crs#'+str(i)+'.png')\n",
    "        plt.clf()\n",
    "    \n",
    "\n",
    "# 学習後のdenceU-Netによる予測を行う関数\n",
    "def predict():\n",
    "    import cv2\n",
    "\n",
    "    # testDataフォルダ配下にleft_imagesフォルダを置いている\n",
    "    X_test, file_names = load_X('../Preprocess/mhd/', 'test')\n",
    "\n",
    "    input_channel_count = 1\n",
    "    output_channel_count = 1\n",
    "    first_layer_filter_count = 64\n",
    "    network = DenceUNet(input_channel_count, output_channel_count, first_layer_filter_count)\n",
    "    model = network.get_model()\n",
    "    model.load_weights('weights/weights-0crs-11ep-0.02loss.hdf5')\n",
    "    BATCH_SIZE = 8\n",
    "    Y_pred = model.predict(X_test, BATCH_SIZE)\n",
    "    \n",
    "    # 変な方向の断面図でないか、気になる！\n",
    "    print(Y_pred.shape)\n",
    "    \n",
    "    # testDataフォルダ配下にleft_imagesフォルダを置いている\n",
    "    save_path = 'DenceU-Net_DataSet/pred/'\n",
    "    if not os.path.exists(save_path): os.makedirs(save_path)\n",
    "\n",
    "    for i, y in enumerate(Y_pred):\n",
    "        cv2.imwrite(save_path+'prediction' + file_names[i] + '.png', denormalize_y(y))\n",
    "\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "#     train_denceunet()\n",
    "#     predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_denceunet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF2-NT",
   "language": "python",
   "name": "tf2-nt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
