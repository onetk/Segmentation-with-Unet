{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H-dence U-Net\n",
    "---\n",
    "\n",
    "参考\n",
    "- 論文 [H-DenseUNet: Hybrid Densely Connected UNet for Liver and Tumor Segmentation from CT Volumes](https://arxiv.org/pdf/1709.07330.pdf)\n",
    "- 公式 [GitHubリポジトリ](https://github.com/xmengli999/H-DenseUNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../Preprocess/datasets/train-label/\n",
    "!ls ../Preprocess/datasets/train-image/\n",
    "!ls ../Preprocess/datasets/test-label/\n",
    "!ls ../Preprocess/datasets/test-image/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_mhd_files(path, excepts=None, choose=None):\n",
    "    '''\n",
    "    指定ディレクトリの mhd 形式のファイルを読み込み、\n",
    "    1-0. 指定されたファイル (ex. 2番目) を除くファイルリスト作成\n",
    "    1-1. 例の結合された配列 images の作成\n",
    "    1-2. images の各スライスごとに症例番号・スライス番号を格納した image_filesの作成\n",
    "    '''\n",
    "\n",
    "    if (excepts!=None):\n",
    "        \n",
    "        mhd_files = natsorted(glob.glob(path+'*.mhd'))\n",
    "        del mhd_files[excepts]\n",
    "\n",
    "        # 1症例目のみ配列作成のため、単体で読み込み\n",
    "        images = SimpleITK.GetArrayFromImage(SimpleITK.ReadImage(mhd_files[0]))\n",
    "        image_files = []\n",
    "        for i in range(images.shape[0]):\n",
    "            image_files.append(mhd_files[0].split('/')[-1].split('.')[0] +'-'+str(i))    \n",
    "\n",
    "        # 以降の症例\n",
    "        for mhd_name in mhd_files:\n",
    "            mhd_array = SimpleITK.GetArrayFromImage(SimpleITK.ReadImage(mhd_name))\n",
    "            images = np.concatenate([images,mhd_array])\n",
    "            for i in range(mhd_array.shape[0]):\n",
    "                image_files.append(mhd_name.split('/')[-1].split('.')[0] +'-'+str(i))     \n",
    "    \n",
    "    '''\n",
    "    指定ディレクトリの mhd 形式のファイルを読み込み、\n",
    "    1-0. 指定されたファイル ex. 2番目 のみの配列 images の作成\n",
    "    1-1. images の各スライスごとに症例番号・スライス番号を格納した image_filesの作成\n",
    "    '''\n",
    "    if(choose!=None):\n",
    "\n",
    "        mhd_files = natsorted(glob.glob(path+'*.mhd'))\n",
    "        mhd_files = mhd_files[choose]\n",
    "\n",
    "        # 1症例目のみ配列作成のため、単体で読み込み\n",
    "        images = SimpleITK.GetArrayFromImage(SimpleITK.ReadImage(mhd_files))\n",
    "        image_files = []\n",
    "        for i in range(images.shape[0]):\n",
    "            image_files.append(mhd_files.split('/')[-1].split('.')[0] +'-'+str(i))    \n",
    "\n",
    "\n",
    "    return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import SimpleITK\n",
    "import numpy as np\n",
    "from natsort import natsorted\n",
    "from ipywidgets import interact\n",
    "from matplotlib import pylab as plt\n",
    "from skimage.util import random_noise\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "%matplotlib inline\n",
    "\n",
    "hdd = '/mnt/Dataset/TN/datasets/'\n",
    "data_lists = natsorted(os.listdir('../Preprocess/GroundTruth/'))\n",
    "\n",
    "class dataProcess(object):\n",
    "\n",
    "    def __init__(self, out_rows, out_cols, data_path  = \"../Preprocess/mhd/\",\n",
    "                                           label_path = \"../Preprocess/GT/*/\",\n",
    "                                           test_path  = \"../Preprocess/mhd/\",\n",
    "                                           testlabel_path = \"../Preprocess/GT/*/\"):\n",
    "\n",
    "        self.out_rows = out_rows\n",
    "        self.out_cols = out_cols\n",
    "        self.data_path = data_path\n",
    "        self.label_path = label_path\n",
    "        self.test_path = test_path\n",
    "        self.testlabel_path = testlabel_path\n",
    "\n",
    "    def _transform(self,imagein):\n",
    "        image = imagein\n",
    "        resize_image = cv2.resize(image, (512, 512), interpolation=cv2.INTER_NEAREST)\n",
    "        if self.__channels and len(image.shape) < 3:  # make sure images are of shape(h,w,3)\n",
    "            image = np.array([image for i in range(3)])\n",
    "        else:\n",
    "            resize_image = image\n",
    "        return np.array(resize_image)\n",
    "\n",
    "    def horizontal_flip(img):\n",
    "        return img[:,::-1]\n",
    "\n",
    "    def vertical_flip(img):\n",
    "        return img[::-1,:]\n",
    "        \n",
    "    def upscale1_1_img(img):\n",
    "        scale = 563     # floor(512*1.1)\n",
    "        img = cv2.resize(img.astype(np.float64), (scale,scale), interpolation = cv2.INTER_NEAREST)\n",
    "        img = img[25:537,25:537]\n",
    "        img = img.astype(np.int32)\n",
    "        return img\n",
    "\n",
    "    def addnoise(img):\n",
    "        mean = 0\n",
    "        sigma = 50\n",
    "        gauss = np.random.normal(mean,sigma,(512,512))\n",
    "        img = img.astype(np.float64) + gauss\n",
    "        img = img.astype(np.int32)\n",
    "        return img\n",
    "    \n",
    "    def create_train_data(self, excepts=0):\n",
    "        '''\n",
    "        data_pathに存在する各症例を読み込み、rate倍のデータ拡張を行ったものを訓練データセットとする\n",
    "        '''\n",
    "        \n",
    "        if not os.path.exists(hdd+\"trainImages-except-\"+data_lists[excepts]+\".npy\"):\n",
    "            print('Creating training images without #{} ...'.format(excepts))\n",
    "\n",
    "            imgdatas= read_mhd_files(self.data_path, excepts=excepts )\n",
    "            num = len(imgdatas)\n",
    "            rate = 4\n",
    "            final_images = np.ndarray([num*rate,1,512,512],'int32')\n",
    "            imglabels = read_mhd_files(self.label_path, excepts=excepts)\n",
    "            final_label = np.ndarray([num*rate,1,512,512],'int32')\n",
    "            for i in tqdm(range(num)):\n",
    "                final_images[rate*i,0] = imgdatas[i]\n",
    "                final_images[rate*i+1,0] = dataProcess.horizontal_flip(imgdatas[i])\n",
    "                final_images[rate*i+2,0] = dataProcess.vertical_flip(imgdatas[i])\n",
    "                final_images[rate*i+3,0] = dataProcess.upscale1_1_img(imgdatas[i])\n",
    "                final_label[rate*i,0] = imglabels[i]\n",
    "                final_label[rate*i+1,0] = dataProcess.horizontal_flip(imglabels[i])\n",
    "                final_label[rate*i+2,0] = dataProcess.vertical_flip(imglabels[i])\n",
    "                final_label[rate*i+3,0] = dataProcess.upscale1_1_img(imglabels[i])\n",
    "\n",
    "\n",
    "            print(final_images.shape)\n",
    "            print(final_label.shape)\n",
    "            np.save(hdd+\"trainImages-except-\"+data_lists[excepts]+\".npy\",final_images)\n",
    "            np.save(hdd+\"trainMasks-except-\"+data_lists[excepts]+\".npy\", final_label)\n",
    "\n",
    "            print('Saving to train.npy files done.')\n",
    "\n",
    "    def create_test_data(self, choose=0):\n",
    "        \n",
    "        if not os.path.exists(hdd+\"testImages-\"+ data_lists[choose] +\".npy\"):\n",
    "            print(\"Creating testing images with #{} ...\".format(choose))\n",
    "\n",
    "            imgdatas =  read_mhd_files(self.test_path, choose=choose)\n",
    "            num = len(imgdatas)\n",
    "            final_images = np.ndarray([num, 1, 512, 512],'int32')\n",
    "\n",
    "            imglabels =  read_mhd_files(self.testlabel_path, choose=choose)\n",
    "            final_label = np.ndarray([num, 1, 512, 512],'int32')\n",
    "            for i in tqdm(range(num)):\n",
    "                final_images[i, 0] = imgdatas[i]\n",
    "                final_label[i, 0] = imglabels[i]\n",
    "            print(final_images.shape)\n",
    "            print(final_label.shape)\n",
    "            np.save(hdd+\"testImages-\"+ data_lists[choose] +\".npy\", final_images)\n",
    "            np.save(hdd+\"testMasks-\"+ data_lists[choose] +\".npy\", final_label)\n",
    "\n",
    "            print('Saving to test.npy files done.')\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "# datasets = dataProcess(512,512)\n",
    "# datasets.create_train_data(excepts=0)\n",
    "# datasets.create_test_data(choose=0)\n",
    "# datasets = None  # メモリ開放"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### 2. Learning and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "# GPU1つのみの設定\n",
    "if 'tensorflow' == K.backend():\n",
    "    import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.visible_device_list = \"1\"\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train data...\n",
      "preprocessing train data...\n",
      "constructing model...\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 1, 512, 512)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 32, 512, 512) 320         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)      (None, 32, 512, 512) 0           conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 32, 512, 512) 9248        leaky_re_lu_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)      (None, 32, 512, 512) 0           conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 32, 512, 512) 2048        leaky_re_lu_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 32, 256, 256) 0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 64, 256, 256) 18496       max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)      (None, 64, 256, 256) 0           conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 64, 256, 256) 1024        leaky_re_lu_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 64, 256, 256) 36928       batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)      (None, 64, 256, 256) 0           conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 64, 256, 256) 1024        leaky_re_lu_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 64, 128, 128) 0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 128, 128, 128 73856       max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)      (None, 128, 128, 128 0           conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 128, 128, 128 512         leaky_re_lu_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 128, 128, 128 147584      batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)      (None, 128, 128, 128 0           conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 128, 128, 128 512         leaky_re_lu_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 128, 64, 64)  0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 256, 64, 64)  295168      max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)      (None, 256, 64, 64)  0           conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 256, 64, 64)  256         leaky_re_lu_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 256, 64, 64)  590080      batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)      (None, 256, 64, 64)  0           conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 256, 64, 64)  256         leaky_re_lu_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 256, 32, 32)  0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 512, 32, 32)  1180160     max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)      (None, 512, 32, 32)  0           conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 512, 32, 32)  128         leaky_re_lu_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 512, 32, 32)  2359808     batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)      (None, 512, 32, 32)  0           conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 512, 32, 32)  128         leaky_re_lu_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_5 (UpSampling2D)  (None, 512, 64, 64)  0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 768, 64, 64)  0           up_sampling2d_5[0][0]            \n",
      "                                                                 batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 256, 64, 64)  1769728     concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 256, 64, 64)  256         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 256, 64, 64)  590080      batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 256, 64, 64)  256         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_6 (UpSampling2D)  (None, 256, 128, 128 0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 384, 128, 128 0           up_sampling2d_6[0][0]            \n",
      "                                                                 batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 128, 128, 128 442496      concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 128, 128, 128 512         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 128, 128, 128 147584      batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 128, 128, 128 512         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_7 (UpSampling2D)  (None, 128, 256, 256 0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 192, 256, 256 0           up_sampling2d_7[0][0]            \n",
      "                                                                 batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 64, 256, 256) 110656      concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 64, 256, 256) 1024        conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 64, 256, 256) 36928       batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 64, 256, 256) 1024        conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_8 (UpSampling2D)  (None, 64, 512, 512) 0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 96, 512, 512) 0           up_sampling2d_8[0][0]            \n",
      "                                                                 batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 32, 512, 512) 27680       concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 32, 512, 512) 2048        conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 32, 512, 512) 9248        batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 32, 512, 512) 2048        conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 1, 512, 512)  33          batch_normalization_34[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 7,859,649\n",
      "Trainable params: 7,852,865\n",
      "Non-trainable params: 6,784\n",
      "__________________________________________________________________________________________________\n",
      "Fitting model...\n",
      "Train on 10364 samples, validate on 1152 samples\n",
      "Epoch 1/20\n",
      "  868/10364 [=>............................] - ETA: 15:55 - loss: 0.6584 - dice_coef: 0.3416"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-201abf73d1c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0mdatasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# メモリ開放\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcombination\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m     \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcombination\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-201abf73d1c7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(use_existing, combination)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;31m# add validation split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;31m# model.fit(imgs_train, imgs_mask_train, batch_size=4, epochs=30, verbose=1, validation_split = 0.1, shuffle=True, callbacks=[model_checkpoint])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgs_mask_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TF2-NT/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/TF2-NT/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TF2-NT/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TF2-NT/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TF2-NT/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, LeakyReLU, UpSampling2D, MaxPooling2D, concatenate, BatchNormalization, Dropout\n",
    "from keras import optimizers\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "import SimpleITK as sitk\n",
    "\n",
    "K.set_image_dim_ordering('th')  # Theano dimension ordering in this code\n",
    "\n",
    "img_rows = 512\n",
    "img_cols = 512\n",
    "smooth = 1.\n",
    "\n",
    "\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return ((2.0 * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth))\n",
    "\n",
    "\n",
    "def dice_coef_np(y_true, y_pred):\n",
    "    y_true_f = y_true.flatten()\n",
    "    y_pred_f = y_pred.flatten()\n",
    "    intersection = np.sum(y_true_f * y_pred_f)\n",
    "    return (2.0 * intersection + smooth) / (np.sum(y_true_f) + np.sum(y_pred_f) + smooth)\n",
    "\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1.0-dice_coef(y_true, y_pred)\n",
    "\n",
    "\n",
    "def get_unet():\n",
    "    inputs = Input((1, img_rows, img_cols))\n",
    "    #conv1 = Convolution2D(32, 3, 3, activation='relu', border_mode='same')(inputs)\n",
    "    #conv1 = Convolution2D(32, 3, 3, activation='relu', border_mode='same')(conv1)\n",
    "    #pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    \n",
    "    conv1 = Conv2D(32, (3,3), padding = 'same')(inputs)\n",
    "    conv1 = LeakyReLU(alpha=0.2)(conv1)\n",
    "    #conv1 = BatchNormalization(momentum = 0.8)(conv1)\n",
    "    conv1 = Conv2D(32, (3,3), padding = 'same')(conv1)\n",
    "    conv1 = LeakyReLU(alpha=0.2)(conv1)\n",
    "    conv1 = BatchNormalization(momentum = 0.8)(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    \n",
    "    #conv2 = Convolution2D(64, 3, 3, activation='relu', border_mode='same')(pool1)\n",
    "    #conv2 = Convolution2D(64, 3, 3, activation='relu', border_mode='same')(conv2)\n",
    "    #pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv2 = Conv2D(64, (3,3), padding = 'same')(pool1)\n",
    "    conv2 = LeakyReLU(alpha=0.2)(conv2)\n",
    "    conv2 = BatchNormalization(momentum = 0.8)(conv2)\n",
    "    conv2 = Conv2D(64, (3,3), padding = 'same')(conv2)\n",
    "    conv2 = LeakyReLU(alpha=0.2)(conv2)\n",
    "    conv2 = BatchNormalization(momentum = 0.8)(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    \n",
    "    #conv3 = Convolution2D(128, 3, 3, activation='relu', border_mode='same')(pool2)\n",
    "    #conv3 = Convolution2D(128, 3, 3, activation='relu', border_mode='same')(conv3)\n",
    "    #pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv3 = Conv2D(128, (3,3), padding = 'same')(pool2)\n",
    "    conv3 = LeakyReLU(alpha=0.2)(conv3)\n",
    "    conv3 = BatchNormalization(momentum = 0.8)(conv3)\n",
    "    conv3 = Conv2D(128, (3,3), padding = 'same')(conv3)\n",
    "    conv3 = LeakyReLU(alpha=0.2)(conv3)\n",
    "    conv3 = BatchNormalization(momentum = 0.8)(conv3)    \n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    \n",
    "    #conv4 = Convolution2D(256, 3, 3, activation='relu', border_mode='same')(pool3)\n",
    "    #conv4 = Convolution2D(256, 3, 3, activation='relu', border_mode='same')(conv4)\n",
    "    #pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    conv4 = Conv2D(256, (3,3), padding = 'same')(pool3)\n",
    "    conv4 = LeakyReLU(alpha=0.2)(conv4)\n",
    "    conv4 = BatchNormalization(momentum = 0.8)(conv4)\n",
    "    conv4 = Conv2D(256, (3,3), padding = 'same')(conv4)\n",
    "    conv4 = LeakyReLU(alpha=0.2)(conv4)\n",
    "    conv4 = BatchNormalization(momentum = 0.8)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "    \n",
    "    #conv5 = Convolution2D(512, 3, 3, activation='relu', border_mode='same')(pool4)\n",
    "    #conv5 = Convolution2D(512, 3, 3, activation='relu', border_mode='same')(conv5)\n",
    "    conv5 = Conv2D(512, (3,3), padding = 'same')(pool4)\n",
    "    conv5 = LeakyReLU(alpha=0.2)(conv5)\n",
    "    conv5 = BatchNormalization(momentum = 0.8)(conv5)\n",
    "    conv5 = Conv2D(512, (3,3), padding = 'same')(conv5)\n",
    "    conv5 = LeakyReLU(alpha=0.2)(conv5)\n",
    "    conv5 = BatchNormalization(momentum = 0.8)(conv5)\n",
    "    ##up6 = merge([UpSampling2D(size=(2, 2))(conv5), conv4], mode='concat', concat_axis=1) %merge not exist in this version\n",
    "    #up6 = concatenate([UpSampling2D(size=(2, 2))(conv5), conv4], axis = 1)\n",
    "    #conv6 = Convolution2D(256, 3, 3, activation='relu', border_mode='same')(up6)\n",
    "    #conv6 = Convolution2D(256, 3, 3, activation='relu', border_mode='same')(conv6)\n",
    "\n",
    "    up6 = concatenate([UpSampling2D(size=(2, 2))(conv5), conv4], axis = 1)\n",
    "    conv6 = Conv2D(256, (3,3), padding = 'same', activation='relu')(up6)\n",
    "    #conv6 = Dropout(rate = 0.5)(conv6)   -> decrease the accuracy so removed \n",
    "    conv6 = BatchNormalization(momentum = 0.8)(conv6)\n",
    "    conv6 = Conv2D(256, (3,3), padding = 'same', activation='relu')(conv6)\n",
    "    #conv6 = Dropout(rate = 0.5)(conv6)   -> decrease the accuracy so removed \n",
    "    conv6 = BatchNormalization(momentum = 0.8)(conv6)\n",
    "\n",
    "    ## up7 = merge([UpSampling2D(size=(2, 2))(conv6), conv3], mode='concat', concat_axis=1)\n",
    "    #up7 = concatenate([UpSampling2D(size=(2, 2))(conv6), conv3], axis = 1)\n",
    "    #conv7 = Convolution2D(128, 3, 3, activation='relu', border_mode='same')(up7)\n",
    "    #conv7 = Convolution2D(128, 3, 3, activation='relu', border_mode='same')(conv7)\n",
    "\n",
    "    up7 = concatenate([UpSampling2D(size=(2, 2))(conv6), conv3], axis = 1)\n",
    "    conv7 = Conv2D(128, (3,3), padding = 'same', activation='relu')(up7)\n",
    "    conv7 = BatchNormalization(momentum = 0.8)(conv7)\n",
    "    conv7 = Conv2D(128, (3,3), padding = 'same', activation='relu')(conv7)\n",
    "    conv7 = BatchNormalization(momentum = 0.8)(conv7)\n",
    "    ##up8 = merge([UpSampling2D(size=(2, 2))(conv7), conv2], mode='concat', concat_axis=1)\n",
    "    #up8 = concatenate([UpSampling2D(size=(2, 2))(conv7), conv2], axis=1)\n",
    "    #conv8 = Convolution2D(64, 3, 3, activation='relu', border_mode='same')(up8)\n",
    "    #conv8 = Convolution2D(64, 3, 3, activation='relu', border_mode='same')(conv8)\n",
    "    up8 = concatenate([UpSampling2D(size=(2, 2))(conv7), conv2], axis = 1)\n",
    "    conv8 = Conv2D(64, (3,3), padding = 'same', activation='relu')(up8)\n",
    "    conv8 = BatchNormalization(momentum = 0.8)(conv8)\n",
    "    conv8 = Conv2D(64, (3,3), padding = 'same', activation='relu')(conv8)\n",
    "    conv8 = BatchNormalization(momentum = 0.8)(conv8)\n",
    "\n",
    "    ##up9 = merge([UpSampling2D(size=(2, 2))(conv8), conv1], mode='concat', concat_axis=1)\n",
    "    #up9 = concatenate([UpSampling2D(size=(2, 2))(conv8), conv1], axis=1)\n",
    "    #conv9 = Convolution2D(32, 3, 3, activation='relu', border_mode='same')(up9)\n",
    "    #conv9 = Convolution2D(32, 3, 3, activation='relu', border_mode='same')(conv9)\n",
    "    up9 = concatenate([UpSampling2D(size=(2, 2))(conv8), conv1], axis = 1)\n",
    "    conv9 = Conv2D(32, (3,3), padding = 'same', activation='relu')(up9)\n",
    "    conv9 = BatchNormalization(momentum = 0.8)(conv9)\n",
    "    conv9 = Conv2D(32, (3,3), padding = 'same', activation='relu')(conv9)\n",
    "    conv9 = BatchNormalization(momentum = 0.8)(conv9)\n",
    "    #conv10 = Convolution2D(1, 1, 1, activation='sigmoid')(conv9)\n",
    "    conv10 = Conv2D(1, (1, 1), activation='sigmoid')(conv9)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=conv10)\n",
    "\n",
    "    #model.compile(optimizer=Adam(lr=1.0e-5), loss=dice_coef_loss, metrics=[dice_coef])  #original\n",
    "    #model.compile(optimizer=Adam(lr=0.001, ), loss=dice_coef_loss, metrics=[dice_coef])\n",
    "    sgd = optimizers.SGD(lr=0.01, decay=1e-4, momentum=0.9, nesterov=True)\n",
    "    model.compile(optimizer=sgd, loss=dice_coef_loss, metrics=[dice_coef])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def train(use_existing, combination=0):\n",
    "        \n",
    "    # --------------- Training ----------------- #\n",
    "    print('Loading train data...')\n",
    "    tag = natsorted(os.listdir('../Preprocess/GroundTruth/'))[combination]\n",
    "\n",
    "    imgs_train = np.load(hdd+\"trainImages-except-\"+ data_lists[combination] +\".npy\").astype(np.float32)\n",
    "    imgs_mask_train = np.load(hdd+\"trainMasks-except-\"+ data_lists[combination] +\".npy\").astype(np.float32)\n",
    "\n",
    "    print('preprocessing train data...')\n",
    "\n",
    "    mean = np.mean(imgs_train)  # mean for data centering\n",
    "    std = np.std(imgs_train)  # std for data normalization\n",
    "\n",
    "    imgs_train -= mean  # images should already be standardized, but just in case\n",
    "    imgs_train /= std\n",
    "\n",
    "    print('constructing model...')\n",
    "\n",
    "    model = get_unet()\n",
    "    model_checkpoint = ModelCheckpoint('unet-'+tag+'.hdf5', monitor='loss', save_best_only=True)\n",
    "\n",
    "    if use_existing:\n",
    "        model.load_weights('./unet-'+tag+'.hdf5')\n",
    "\n",
    "    print('Fitting model...')\n",
    "\n",
    "    # add validation split\n",
    "    # model.fit(imgs_train, imgs_mask_train, batch_size=4, epochs=30, verbose=1, validation_split = 0.1, shuffle=True, callbacks=[model_checkpoint])\n",
    "    model.fit(imgs_train, imgs_mask_train, batch_size=4, epochs=20, verbose=1, validation_split = 0.1, shuffle=True, callbacks=[model_checkpoint])\n",
    "\n",
    "    \n",
    "    # plot figure\n",
    "    #     plt.plot(range(1, NUM_EPOCH+1), learning_history.history['dice_coef'], label=\"training\")\n",
    "    #     # plt.plot(range(1, NUM_EPOCH+1), learning_history.history['val_dice_coef'], label=\"validation\")\n",
    "    #     plt.xlabel('Epochs')\n",
    "    #     plt.ylabel('Accuracy')\n",
    "    #     plt.legend()\n",
    "    #     plt.ylim(0,1)\n",
    "    #     plt.grid(axis='y', color='lightgray')\n",
    "    #     plt.savefig('progress-'+str(combination)+'.png')\n",
    "    #     plt.clf()\n",
    "    \n",
    "def predict(use_existing, combination=0):\n",
    "\n",
    "    # --------------- Predict ----------------- #\n",
    "    print('Loading test data...')\n",
    "    \n",
    "    #tag = glob.glob(mydata.testlabel_path)[0].split('/')[-2]\n",
    "    tag = natsorted(os.listdir('../Preprocess/GroundTruth/'))[combination]\n",
    "    imgs_test = np.load(hdd+\"testImages-\"+ data_lists[combination] +\".npy\").astype(np.float32)\n",
    "    imgs_mask = np.load(hdd+\"testMasks-\"+ data_lists[combination] +\".npy\").astype(np.float32)\n",
    "    \n",
    "    print('preprocessing test data...')\n",
    "\n",
    "    mean = np.mean(imgs_test)  # mean for data centering\n",
    "    std = np.std(imgs_test)  # std for data normalization\n",
    "    \n",
    "    imgs_test -= mean  # images should already be standardized, but just in case\n",
    "    imgs_test /= std      \n",
    "    \n",
    "    model = get_unet()\n",
    "    model.load_weights('./unet-'+tag+'.hdf5')\n",
    "   \n",
    "    num_test = len(imgs_test)\n",
    "    imgs_mask_test = np.ndarray([num_test, 1, 512, 512], dtype=np.float32)\n",
    "    for i in range(num_test):\n",
    "        imgs_mask_test[i] = model.predict([imgs_test[i:i + 1]], verbose=0)[0]\n",
    "    np.save(tag+'_masksTestPredict.npy', imgs_mask_test)\n",
    "    mean = 0.0\n",
    "    for i in range(num_test):\n",
    "        mean += dice_coef_np(imgs_mask[i, 0], imgs_mask_test[i, 0])\n",
    "    mean /= num_test\n",
    "    print(\"Mean Dice Coeff : \", mean)\n",
    "    \n",
    "    img = sitk.GetImageFromArray(np.reshape(imgs_mask_test,(num_test,512,512)),isVector=False)\n",
    "    img.SetSpacing([0.351562, 0.351562, 0.625]) #ElementType\n",
    "    img.SetOrigin([-60, -110, -175]) #offset\n",
    "    sitk.WriteImage(img, './result/result_mask_'+tag+'_{:.2f}dice.mhd'.format(mean))\n",
    "    # imgs_mask_test = np.squeeze(imgs_mask_test, axis=3)\n",
    "    # np.save('imgs_mask_test.npy', imgs_mask_test)\n",
    "\n",
    "\n",
    "    \n",
    "# train_and_predict(False)\n",
    "\n",
    "\n",
    "for index in range(len(os.listdir('../Preprocess/GroundTruth/'))):\n",
    "    # データセット逐次生成\n",
    "    datasets = dataProcess(512,512)\n",
    "    datasets.create_train_data(excepts=0)\n",
    "    datasets.create_test_data(choose=0)\n",
    "    datasets = None  # メモリ開放\n",
    "\n",
    "    train(False, combination=index)\n",
    "    predict(False, combination=index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Check Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import SimpleITK\n",
    "from ipywidgets import interact\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "test  = SimpleITK.ReadImage(glob.glob('./result/*.mhd')[0])\n",
    "label = SimpleITK.ReadImage(glob.glob('../Preprocess/datasets/test-label/*/*.mhd')[0])\n",
    "test_array  = SimpleITK.GetArrayFromImage(test)\n",
    "label_array = SimpleITK.GetArrayFromImage(label)\n",
    "\n",
    "plt.figure(figsize=(20, 20), dpi=25)\n",
    "plt.rcParams['figure.figsize'] = (10.0, 5.0)\n",
    "\n",
    "@interact(slices=(0,label_array.shape[0]-1))\n",
    "def plot_rolling_mean(slices=140):    \n",
    "    plt.subplot(121)\n",
    "    plt.imshow(label_array[slices, :, :])\n",
    "    plt.title('GT')\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(test_array[slices, :, :])\n",
    "    plt.title('predict')\n",
    "    \n",
    "    plt.gray()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['L0',\n",
       " 'L1',\n",
       " 'L3',\n",
       " 'L4',\n",
       " 'M0',\n",
       " 'M1',\n",
       " 'M2',\n",
       " 'M3',\n",
       " 'M4',\n",
       " 'M5',\n",
       " 'S0',\n",
       " 'S1',\n",
       " 'S2',\n",
       " 'S3',\n",
       " 'S4']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "natsorted(os.listdir('../Preprocess/GroundTruth/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tes(excepts=None, choose=None):\n",
    "    if (excepts) :print(excepts)\n",
    "\n",
    "tes(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=0.005\n",
    "print('{:.2f}'.format(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ADNI', 'Yolo', 'BrainWeb', 'datasets', 'backup', 'pix2pix', 'Git', 'AI-edge']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('/mnt/Dataset/TN/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF2-NT",
   "language": "python",
   "name": "tf2-nt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
