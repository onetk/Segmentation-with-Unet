{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルの作成\n",
    "---\n",
    "[参考](http://ni4muraano.hatenablog.com/entry/2017/08/10/101053)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers.convolutional import Conv2D, ZeroPadding2D, Conv2DTranspose\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers import LeakyReLU, BatchNormalization, Activation, Dropout\n",
    "\n",
    "class UNet(object):\n",
    "    def __init__(self, input_channel_count, output_channel_count, first_layer_filter_count):\n",
    "        self.INPUT_IMAGE_SIZE = 256\n",
    "        self.CONCATENATE_AXIS = -1\n",
    "        self.CONV_FILTER_SIZE = 4\n",
    "        self.CONV_STRIDE = 2\n",
    "        self.CONV_PADDING = (1, 1)\n",
    "        self.DECONV_FILTER_SIZE = 2\n",
    "        self.DECONV_STRIDE = 2\n",
    "\n",
    "        # (256 x 256 x input_channel_count)\n",
    "        inputs = Input((self.INPUT_IMAGE_SIZE, self.INPUT_IMAGE_SIZE, input_channel_count))\n",
    "\n",
    "        # エンコーダーの作成\n",
    "        # (128 x 128 x N)\n",
    "        enc1 = ZeroPadding2D(self.CONV_PADDING)(inputs)\n",
    "        enc1 = Conv2D(first_layer_filter_count, self.CONV_FILTER_SIZE, strides=self.CONV_STRIDE)(enc1)\n",
    "\n",
    "        # (64 x 64 x 2N)\n",
    "        filter_count = first_layer_filter_count*2\n",
    "        enc2 = self._add_encoding_layer(filter_count, enc1)\n",
    "\n",
    "        # (32 x 32 x 4N)\n",
    "        filter_count = first_layer_filter_count*4\n",
    "        enc3 = self._add_encoding_layer(filter_count, enc2)\n",
    "\n",
    "        # (16 x 16 x 8N)\n",
    "        filter_count = first_layer_filter_count*8\n",
    "        enc4 = self._add_encoding_layer(filter_count, enc3)\n",
    "\n",
    "        # (8 x 8 x 8N)\n",
    "        enc5 = self._add_encoding_layer(filter_count, enc4)\n",
    "\n",
    "        # (4 x 4 x 8N)\n",
    "        enc6 = self._add_encoding_layer(filter_count, enc5)\n",
    "\n",
    "        # (2 x 2 x 8N)\n",
    "        enc7 = self._add_encoding_layer(filter_count, enc6)\n",
    "\n",
    "        # (1 x 1 x 8N)\n",
    "        enc8 = self._add_encoding_layer(filter_count, enc7)\n",
    "\n",
    "        # デコーダーの作成\n",
    "        # (2 x 2 x 8N)\n",
    "        dec1 = self._add_decoding_layer(filter_count, True, enc8)\n",
    "        dec1 = concatenate([dec1, enc7], axis=self.CONCATENATE_AXIS)\n",
    "\n",
    "        # (4 x 4 x 8N)\n",
    "        dec2 = self._add_decoding_layer(filter_count, True, dec1)\n",
    "        dec2 = concatenate([dec2, enc6], axis=self.CONCATENATE_AXIS)\n",
    "\n",
    "        # (8 x 8 x 8N)\n",
    "        dec3 = self._add_decoding_layer(filter_count, True, dec2)\n",
    "        dec3 = concatenate([dec3, enc5], axis=self.CONCATENATE_AXIS)\n",
    "\n",
    "        # (16 x 16 x 8N)\n",
    "        dec4 = self._add_decoding_layer(filter_count, False, dec3)\n",
    "        dec4 = concatenate([dec4, enc4], axis=self.CONCATENATE_AXIS)\n",
    "\n",
    "        # (32 x 32 x 4N)\n",
    "        filter_count = first_layer_filter_count*4\n",
    "        dec5 = self._add_decoding_layer(filter_count, False, dec4)\n",
    "        dec5 = concatenate([dec5, enc3], axis=self.CONCATENATE_AXIS)\n",
    "\n",
    "        # (64 x 64 x 2N)\n",
    "        filter_count = first_layer_filter_count*2\n",
    "        dec6 = self._add_decoding_layer(filter_count, False, dec5)\n",
    "        dec6 = concatenate([dec6, enc2], axis=self.CONCATENATE_AXIS)\n",
    "\n",
    "        # (128 x 128 x N)\n",
    "        filter_count = first_layer_filter_count\n",
    "        dec7 = self._add_decoding_layer(filter_count, False, dec6)\n",
    "        dec7 = concatenate([dec7, enc1], axis=self.CONCATENATE_AXIS)\n",
    "\n",
    "        # (256 x 256 x output_channel_count)\n",
    "        dec8 = Activation(activation='relu')(dec7)\n",
    "        dec8 = Conv2DTranspose(output_channel_count, self.DECONV_FILTER_SIZE, strides=self.DECONV_STRIDE)(dec8)\n",
    "        dec8 = Activation(activation='sigmoid')(dec8)\n",
    "\n",
    "        self.UNET = Model(input=inputs, output=dec8)\n",
    "\n",
    "    def _add_encoding_layer(self, filter_count, sequence):\n",
    "        new_sequence = LeakyReLU(0.2)(sequence)\n",
    "        new_sequence = ZeroPadding2D(self.CONV_PADDING)(new_sequence)\n",
    "        new_sequence = Conv2D(filter_count, self.CONV_FILTER_SIZE, strides=self.CONV_STRIDE)(new_sequence)\n",
    "        new_sequence = BatchNormalization()(new_sequence)\n",
    "        return new_sequence\n",
    "\n",
    "    def _add_decoding_layer(self, filter_count, add_drop_layer, sequence):\n",
    "        new_sequence = Activation(activation='relu')(sequence)\n",
    "        new_sequence = Conv2DTranspose(filter_count, self.DECONV_FILTER_SIZE, strides=self.DECONV_STRIDE,\n",
    "                                       kernel_initializer='he_uniform')(new_sequence)\n",
    "        new_sequence = BatchNormalization()(new_sequence)\n",
    "        if add_drop_layer:\n",
    "            new_sequence = Dropout(0.5)(new_sequence)\n",
    "        return new_sequence\n",
    "\n",
    "    def get_model(self):\n",
    "        return self.UNET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### その他関数定義\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 256\n",
    "\n",
    "# 値を-1から1に正規化する関数\n",
    "def normalize_x(image):\n",
    "    image = image/127.5 - 1\n",
    "    return image\n",
    "\n",
    "\n",
    "# 値を0から1に正規化する関数\n",
    "def normalize_y(image):\n",
    "    image = image/255\n",
    "    return image\n",
    "\n",
    "\n",
    "# 値を0から255に戻す関数\n",
    "def denormalize_y(image):\n",
    "    image = image*255\n",
    "    return image\n",
    "\n",
    "\n",
    "# インプット画像を読み込む関数\n",
    "def load_X(folder_path):\n",
    "    import os, cv2\n",
    "\n",
    "    image_files = os.listdir(folder_path)\n",
    "    image_files.sort()\n",
    "    images = np.zeros((len(image_files), IMAGE_SIZE, IMAGE_SIZE, 3), np.float32)\n",
    "    for i, image_file in enumerate(image_files):\n",
    "        image = cv2.imread(folder_path + os.sep + image_file)\n",
    "        image = cv2.resize(image, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "        images[i] = normalize_x(image)\n",
    "    return images, image_files\n",
    "\n",
    "\n",
    "# ラベル画像を読み込む関数\n",
    "def load_Y(folder_path):\n",
    "    import os, cv2\n",
    "\n",
    "    image_files = os.listdir(folder_path)\n",
    "    image_files.sort()\n",
    "    images = np.zeros((len(image_files), IMAGE_SIZE, IMAGE_SIZE, 1), np.float32)\n",
    "    for i, image_file in enumerate(image_files):\n",
    "        image = cv2.imread(folder_path + os.sep + image_file, cv2.IMREAD_GRAYSCALE)\n",
    "        image = cv2.resize(image, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "        image = image[:, :, np.newaxis]\n",
    "        images[i] = normalize_y(image)\n",
    "    return images\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train/test部分\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'trainingData/left_images'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-515056162f35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mtrain_unet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;31m#     predict()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-515056162f35>\u001b[0m in \u001b[0;36mtrain_unet\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_unet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# trainingDataフォルダ配下にleft_imagesフォルダを置いている\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'trainingData'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'left_images'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;31m# trainingDataフォルダ配下にleft_groundTruthフォルダを置いている\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mY_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_Y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'trainingData'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'left_groundTruth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-527d5ad9decd>\u001b[0m in \u001b[0;36mload_X\u001b[0;34m(folder_path)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mimage_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mimage_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMAGE_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMAGE_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'trainingData/left_images'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from unet import UNet\n",
    "\n",
    "# ダイス係数を計算する関数\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true = K.flatten(y_true)\n",
    "    y_pred = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true * y_pred)\n",
    "    return 2.0 * intersection / (K.sum(y_true) + K.sum(y_pred) + 1)\n",
    "\n",
    "\n",
    "# ロス関数\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1.0 - dice_coef(y_true, y_pred)\n",
    "\n",
    "\n",
    "# U-Netのトレーニングを実行する関数\n",
    "def train_unet():\n",
    "    # trainingDataフォルダ配下にleft_imagesフォルダを置いている\n",
    "    X_train, file_names = load_X('trainingData' + os.sep + 'left_images')\n",
    "    # trainingDataフォルダ配下にleft_groundTruthフォルダを置いている\n",
    "    Y_train = load_Y('trainingData' + os.sep + 'left_groundTruth')\n",
    "\n",
    "    # 入力はBGR3チャンネル\n",
    "    input_channel_count = 3\n",
    "    # 出力はグレースケール1チャンネル\n",
    "    output_channel_count = 1\n",
    "    # 一番初めのConvolutionフィルタ枚数は64\n",
    "    first_layer_filter_count = 64\n",
    "    # U-Netの生成\n",
    "    network = UNet(input_channel_count, output_channel_count, first_layer_filter_count)\n",
    "    model = network.get_model()\n",
    "    model.compile(loss=dice_coef_loss, optimizer=Adam(), metrics=[dice_coef])\n",
    "    \n",
    "    model.summary()\n",
    "\n",
    "    BATCH_SIZE = 12\n",
    "    # 20エポック回せば十分\n",
    "    NUM_EPOCH = 20\n",
    "    history = model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=NUM_EPOCH, verbose=1)\n",
    "    model.save_weights('unet_weights.hdf5')\n",
    "\n",
    "\n",
    "# 学習後のU-Netによる予測を行う関数\n",
    "def predict():\n",
    "    import cv2\n",
    "\n",
    "    # testDataフォルダ配下にleft_imagesフォルダを置いている\n",
    "    X_test, file_names = load_X('testData' + os.sep + 'left_images')\n",
    "\n",
    "    input_channel_count = 3\n",
    "    output_channel_count = 1\n",
    "    first_layer_filter_count = 64\n",
    "    network = UNet(input_channel_count, output_channel_count, first_layer_filter_count)\n",
    "    model = network.get_model()\n",
    "    model.load_weights('unet_weights.hdf5')\n",
    "    BATCH_SIZE = 12\n",
    "    Y_pred = model.predict(X_test, BATCH_SIZE)\n",
    "\n",
    "    for i, y in enumerate(Y_pred):\n",
    "        # testDataフォルダ配下にleft_imagesフォルダを置いている\n",
    "        img = cv2.imread('testData' + os.sep + 'left_images' + os.sep + file_names[i])\n",
    "        y = cv2.resize(y, (img.shape[1], img.shape[0]))\n",
    "        cv2.imwrite('prediction' + str(i) + '.png', denormalize_y(y))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_unet()\n",
    "#     predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Preprocess/mhd/L0.mhd  ../Preprocess/mhd/M1.mhd  ../Preprocess/mhd/S0.mhd\n",
      "../Preprocess/mhd/L1.mhd  ../Preprocess/mhd/M2.mhd  ../Preprocess/mhd/S1.mhd\n",
      "../Preprocess/mhd/L3.mhd  ../Preprocess/mhd/M3.mhd  ../Preprocess/mhd/S2.mhd\n",
      "../Preprocess/mhd/L4.mhd  ../Preprocess/mhd/M4.mhd  ../Preprocess/mhd/S3.mhd\n",
      "../Preprocess/mhd/M0.mhd  ../Preprocess/mhd/M5.mhd  ../Preprocess/mhd/S4.mhd\n"
     ]
    }
   ],
   "source": [
    "!ls ../Preprocess/mhd/*.mhd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Preprocess/GroundTruth/L0/okada1.mhd\n",
      "../Preprocess/GroundTruth/L0/okada2.mhd\n",
      "../Preprocess/GroundTruth/L1/L1 maeda1.mhd\n",
      "../Preprocess/GroundTruth/L1/L1 maeda2.mhd\n",
      "../Preprocess/GroundTruth/L3/L3 maeda1.mhd\n",
      "../Preprocess/GroundTruth/L3/L3 maeda2.mhd\n",
      "../Preprocess/GroundTruth/L4/L4 maeda1.mhd\n",
      "../Preprocess/GroundTruth/L4/L4 maeda2.mhd\n",
      "../Preprocess/GroundTruth/M0/okada.mhd\n",
      "../Preprocess/GroundTruth/M0/okada2.mhd\n",
      "../Preprocess/GroundTruth/M1/okada1.mhd\n",
      "../Preprocess/GroundTruth/M1/okada2.mhd\n",
      "../Preprocess/GroundTruth/M2/okada1.mhd\n",
      "../Preprocess/GroundTruth/M2/okada2.mhd\n",
      "../Preprocess/GroundTruth/M3/M3 maeda1.mhd\n",
      "../Preprocess/GroundTruth/M3/M3 maeda2.mhd\n",
      "../Preprocess/GroundTruth/M4/M4 maeda1.mhd\n",
      "../Preprocess/GroundTruth/M4/M4 maeda2.mhd\n",
      "../Preprocess/GroundTruth/M5/M5 maeda1.mhd\n",
      "../Preprocess/GroundTruth/M5/M5 maeda2.mhd\n",
      "../Preprocess/GroundTruth/S0/okada1.mhd\n",
      "../Preprocess/GroundTruth/S0/okada2.mhd\n",
      "../Preprocess/GroundTruth/S1/okada.mhd\n",
      "../Preprocess/GroundTruth/S1/okada2.mhd\n",
      "../Preprocess/GroundTruth/S2/okada1.mhd\n",
      "../Preprocess/GroundTruth/S2/okada2.mhd\n",
      "../Preprocess/GroundTruth/S3/S3 maeda1.mhd\n",
      "../Preprocess/GroundTruth/S3/S3 maeda2.mhd\n",
      "../Preprocess/GroundTruth/S4/S4 maeda1.mhd\n",
      "../Preprocess/GroundTruth/S4/S4 maeda2.mhd\n"
     ]
    }
   ],
   "source": [
    "!ls ../Preprocess/GroundTruth/*/*.mhd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF2-NT",
   "language": "python",
   "name": "tf2-nt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
