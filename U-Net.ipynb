{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルの作成\n",
    "---\n",
    "[参考](http://ni4muraano.hatenablog.com/entry/2017/08/10/101053)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers.convolutional import Conv2D, ZeroPadding2D, Conv2DTranspose\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers import LeakyReLU, BatchNormalization, Activation, Dropout\n",
    "\n",
    "class UNet(object):\n",
    "    def __init__(self, input_channel_count, output_channel_count, first_layer_filter_count):\n",
    "        self.INPUT_IMAGE_SIZE = 512\n",
    "        self.CONCATENATE_AXIS = -1\n",
    "        self.CONV_FILTER_SIZE = 4\n",
    "        self.CONV_STRIDE = 2\n",
    "        self.CONV_PADDING = (1, 1)\n",
    "        self.DECONV_FILTER_SIZE = 2\n",
    "        self.DECONV_STRIDE = 2\n",
    "\n",
    "        # (512 x 512 x input_channel_count)\n",
    "        inputs = Input((self.INPUT_IMAGE_SIZE, self.INPUT_IMAGE_SIZE, input_channel_count))\n",
    "\n",
    "        # エンコーダーの作成\n",
    "        # (256 x 256 x N)\n",
    "        enc1 = ZeroPadding2D(self.CONV_PADDING)(inputs)\n",
    "        enc1 = Conv2D(first_layer_filter_count, self.CONV_FILTER_SIZE, strides=self.CONV_STRIDE)(enc1)\n",
    "\n",
    "        # (128 x 128 x 2N)\n",
    "        filter_count = first_layer_filter_count*2\n",
    "        enc2 = self._add_encoding_layer(filter_count, enc1)\n",
    "\n",
    "        # (64 x 64 x 4N)\n",
    "        filter_count = first_layer_filter_count*4\n",
    "        enc3 = self._add_encoding_layer(filter_count, enc2)\n",
    "\n",
    "        # (32 x 32 x 8N)\n",
    "        filter_count = first_layer_filter_count*8\n",
    "        enc4 = self._add_encoding_layer(filter_count, enc3)\n",
    "\n",
    "        # (16 x 16 x 8N)\n",
    "        enc5 = self._add_encoding_layer(filter_count, enc4)\n",
    "\n",
    "        # (8 x 8 x 8N)\n",
    "        enc6 = self._add_encoding_layer(filter_count, enc5)\n",
    "\n",
    "        # (4 x 4 x 8N)\n",
    "        enc7 = self._add_encoding_layer(filter_count, enc6)\n",
    "\n",
    "        # (2 x 2 x 8N)\n",
    "        enc8 = self._add_encoding_layer(filter_count, enc7)\n",
    "\n",
    "        # (1 x 1 x 8N)\n",
    "        enc9 = self._add_encoding_layer(filter_count, enc8)\n",
    "        \n",
    "        # デコーダーの作成\n",
    "        # (2 x 2 x 8N)\n",
    "        dec1 = self._add_decoding_layer(filter_count, True, enc9)\n",
    "        dec1 = concatenate([dec1, enc8], axis=self.CONCATENATE_AXIS)\n",
    "\n",
    "        # (4 x 4 x 8N)\n",
    "        dec2 = self._add_decoding_layer(filter_count, True, dec1)\n",
    "        dec2 = concatenate([dec2, enc7], axis=self.CONCATENATE_AXIS)\n",
    "\n",
    "        # (8 x 8 x 8N)\n",
    "        dec3 = self._add_decoding_layer(filter_count, True, dec2)\n",
    "        dec3 = concatenate([dec3, enc6], axis=self.CONCATENATE_AXIS)\n",
    "\n",
    "        # (16 x 16 x 8N)\n",
    "        dec4 = self._add_decoding_layer(filter_count, False, dec3)\n",
    "        dec4 = concatenate([dec4, enc5], axis=self.CONCATENATE_AXIS)\n",
    "\n",
    "        # (32 x 32 x 8N)\n",
    "        dec5 = self._add_decoding_layer(filter_count, False, dec4)\n",
    "        dec5 = concatenate([dec5, enc4], axis=self.CONCATENATE_AXIS)\n",
    "        \n",
    "        # (64 x 64 x 4N)\n",
    "        filter_count = first_layer_filter_count*4\n",
    "        dec6 = self._add_decoding_layer(filter_count, False, dec5)\n",
    "        dec6 = concatenate([dec6, enc3], axis=self.CONCATENATE_AXIS)\n",
    "\n",
    "        # (128 x 128 x 2N)\n",
    "        filter_count = first_layer_filter_count*2\n",
    "        dec7 = self._add_decoding_layer(filter_count, False, dec6)\n",
    "        dec7 = concatenate([dec7, enc2], axis=self.CONCATENATE_AXIS)\n",
    "\n",
    "        # (256 x 256 x N)\n",
    "        filter_count = first_layer_filter_count\n",
    "        dec8 = self._add_decoding_layer(filter_count, False, dec7)\n",
    "        dec8 = concatenate([dec8, enc1], axis=self.CONCATENATE_AXIS)\n",
    "\n",
    "        # (512 x 512 x output_channel_count)\n",
    "        dec9 = Activation(activation='relu')(dec8)\n",
    "        dec9 = Conv2DTranspose(output_channel_count, self.DECONV_FILTER_SIZE, strides=self.DECONV_STRIDE)(dec9)\n",
    "        dec9 = Activation(activation='sigmoid')(dec9)\n",
    "\n",
    "        self.UNET = Model(input=inputs, output=dec9)\n",
    "\n",
    "    def _add_encoding_layer(self, filter_count, sequence):\n",
    "        new_sequence = LeakyReLU(0.2)(sequence)\n",
    "        new_sequence = ZeroPadding2D(self.CONV_PADDING)(new_sequence)\n",
    "        new_sequence = Conv2D(filter_count, self.CONV_FILTER_SIZE, strides=self.CONV_STRIDE)(new_sequence)\n",
    "        new_sequence = BatchNormalization()(new_sequence)\n",
    "        return new_sequence\n",
    "\n",
    "    def _add_decoding_layer(self, filter_count, add_drop_layer, sequence):\n",
    "        new_sequence = Activation(activation='relu')(sequence)\n",
    "        new_sequence = Conv2DTranspose(filter_count, self.DECONV_FILTER_SIZE, strides=self.DECONV_STRIDE,\n",
    "                                       kernel_initializer='he_uniform')(new_sequence)\n",
    "        new_sequence = BatchNormalization()(new_sequence)\n",
    "        if add_drop_layer:\n",
    "            new_sequence = Dropout(0.5)(new_sequence)\n",
    "        return new_sequence\n",
    "\n",
    "    def get_model(self):\n",
    "        return self.UNET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### その他関数定義\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 512\n",
    "TRAIN_PERCENTAGE = 0.8\n",
    "\n",
    "# 値を-1から1に正規化する関数\n",
    "def normalize_x(image):\n",
    "    # image = image/127.5 - 1\n",
    "    image = (image-np.min(image)) / (np.max(image)/2) - 1\n",
    "    return image\n",
    "\n",
    "\n",
    "# 値を0から1に正規化する関数\n",
    "def normalize_y(image):\n",
    "    image = image/255\n",
    "    return image\n",
    "\n",
    "\n",
    "# 値を0から255に戻す関数\n",
    "def denormalize_y(image):\n",
    "    image = image*255\n",
    "    return image\n",
    "\n",
    "\n",
    "# インプット画像を読み込む関数\n",
    "def load_X(folder_path, mode):\n",
    "    import glob, SimpleITK\n",
    "    from natsort import natsorted\n",
    "    \n",
    "    mhd_files = natsorted(glob.glob(folder_path+'*.mhd'))\n",
    "    limits = int(len(mhd_files)*TRAIN_PERCENTAGE)\n",
    "    \n",
    "    if mode=='train':\n",
    "        images = SimpleITK.GetArrayFromImage(SimpleITK.ReadImage(mhd_files[0]))\n",
    "        \n",
    "        image_files = []\n",
    "        for i in range(images.shape[0]):\n",
    "            image_files.append(mhd_files[0].split('/')[-1].split('.')[0] +'-'+str(i))    \n",
    "\n",
    "        for mhd_name in mhd_files[1:int(limits)]:\n",
    "            mhd_array = SimpleITK.GetArrayFromImage(SimpleITK.ReadImage(mhd_name))\n",
    "            images = np.concatenate([images,mhd_array])\n",
    "            for i in range(mhd_array.shape[0]):\n",
    "                image_files.append(mhd_name.split('/')[-1].split('.')[0] +'-'+str(i))     \n",
    "\n",
    "    elif mode=='test':\n",
    "        images = SimpleITK.GetArrayFromImage(SimpleITK.ReadImage(mhd_files[0]))\n",
    "        \n",
    "        image_files = []\n",
    "        for i in range(images.shape[0]):\n",
    "            image_files.append(mhd_files[limits].split('/')[-1].split('.')[0] +'-'+str(i))    \n",
    "\n",
    "        for mhd_name in mhd_files[(int(limits)+1):]:\n",
    "            mhd_array = SimpleITK.GetArrayFromImage(SimpleITK.ReadImage(mhd_name))\n",
    "            images = np.concatenate([images,mhd_array])\n",
    "            for i in range(mhd_array.shape[0]):\n",
    "                image_files.append(mhd_name.split('/')[-1].split('.')[0] +'-'+str(i))  \n",
    "                \n",
    "                \n",
    "    images = normalize_x(images[:, :, :, np.newaxis])\n",
    "\n",
    "    #images = np.zeros((len(image_files), IMAGE_SIZE, IMAGE_SIZE, 1), np.float32)\n",
    "    return images, image_files\n",
    "\n",
    "\n",
    "# ラベル画像を読み込む関数\n",
    "def load_Y(folder_path):\n",
    "    import glob, SimpleITK\n",
    "    from natsort import natsorted\n",
    "    \n",
    "    mhd_files = natsorted(glob.glob(folder_path+'*/*[!2].mhd'))\n",
    "    limits = int(len(mhd_files)*TRAIN_PERCENTAGE)\n",
    "    \n",
    "    images = SimpleITK.GetArrayFromImage(SimpleITK.ReadImage(mhd_files[0]))\n",
    "    for mhd_name in mhd_files[1:int(limits)]:\n",
    "        mhd_array = SimpleITK.GetArrayFromImage(SimpleITK.ReadImage(mhd_name))\n",
    "        images = np.concatenate([images,mhd_array])\n",
    "    \n",
    "    # GTの値の範囲が0-6だったので、1以上は1にしている\n",
    "    images = np.where(images >= 1 , 1 ,images)\n",
    "    return images[:, :, :, np.newaxis]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train/test部分\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "# GPU1つのみの設定\n",
    "if 'tensorflow' == K.backend():\n",
    "    import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.visible_device_list = \"1\"\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dlbox/anaconda3/envs/TF2-NT/lib/python3.6/site-packages/ipykernel_launcher.py:93: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"ac...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 512, 512, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D (None, 514, 514, 1)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 256, 256, 64) 1088        zero_padding2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 256, 256, 64) 0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPadding2D (None, 258, 258, 64) 0           leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 128, 128, 128 131200      zero_padding2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 128, 128 512         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 128, 128, 128 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPadding2D (None, 130, 130, 128 0           leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 64, 64, 256)  524544      zero_padding2d_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64, 64, 256)  1024        conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 64, 64, 256)  0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPadding2D (None, 66, 66, 256)  0           leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 512)  2097664     zero_padding2d_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 512)  2048        conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 32, 32, 512)  0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_5 (ZeroPadding2D (None, 34, 34, 512)  0           leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 16, 16, 512)  4194816     zero_padding2d_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 16, 16, 512)  2048        conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 16, 16, 512)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_6 (ZeroPadding2D (None, 18, 18, 512)  0           leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 8, 8, 512)    4194816     zero_padding2d_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 8, 8, 512)    2048        conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 8, 8, 512)    0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_7 (ZeroPadding2D (None, 10, 10, 512)  0           leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 4, 4, 512)    4194816     zero_padding2d_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 4, 4, 512)    2048        conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 4, 4, 512)    0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_8 (ZeroPadding2D (None, 6, 6, 512)    0           leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 2, 2, 512)    4194816     zero_padding2d_8[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 2, 2, 512)    2048        conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)       (None, 2, 2, 512)    0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_9 (ZeroPadding2D (None, 4, 4, 512)    0           leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 1, 1, 512)    4194816     zero_padding2d_9[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 1, 1, 512)    2048        conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1, 1, 512)    0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 2, 2, 512)    1049088     activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 2, 2, 512)    2048        conv2d_transpose_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 2, 2, 512)    0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 2, 2, 1024)   0           dropout_1[0][0]                  \n",
      "                                                                 batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 2, 2, 1024)   0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 4, 4, 512)    2097664     activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 4, 4, 512)    2048        conv2d_transpose_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 4, 4, 512)    0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 4, 4, 1024)   0           dropout_2[0][0]                  \n",
      "                                                                 batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 4, 4, 1024)   0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 8, 8, 512)    2097664     activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 8, 8, 512)    2048        conv2d_transpose_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 8, 8, 512)    0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 8, 8, 1024)   0           dropout_3[0][0]                  \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 8, 8, 1024)   0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTrans (None, 16, 16, 512)  2097664     activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 512)  2048        conv2d_transpose_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 16, 16, 1024) 0           batch_normalization_12[0][0]     \n",
      "                                                                 batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 16, 16, 1024) 0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTrans (None, 32, 32, 512)  2097664     activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 32, 32, 512)  2048        conv2d_transpose_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 32, 32, 1024) 0           batch_normalization_13[0][0]     \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 1024) 0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_6 (Conv2DTrans (None, 64, 64, 256)  1048832     activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 64, 64, 256)  1024        conv2d_transpose_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 64, 64, 512)  0           batch_normalization_14[0][0]     \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 64, 64, 512)  0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_7 (Conv2DTrans (None, 128, 128, 128 262272      activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 128, 128, 128 512         conv2d_transpose_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 128, 128, 256 0           batch_normalization_15[0][0]     \n",
      "                                                                 batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 128, 128, 256 0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_8 (Conv2DTrans (None, 256, 256, 64) 65600       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 256, 256, 64) 256         conv2d_transpose_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 256, 256, 128 0           batch_normalization_16[0][0]     \n",
      "                                                                 conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 256, 256, 128 0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_9 (Conv2DTrans (None, 512, 512, 1)  513         activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 512, 512, 1)  0           conv2d_transpose_9[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 34,571,393\n",
      "Trainable params: 34,558,465\n",
      "Non-trainable params: 12,928\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/20\n",
      "2308/2308 [==============================] - 79s 34ms/step - loss: 0.8080 - dice_coef: 0.1920\n",
      "Epoch 2/20\n",
      "2308/2308 [==============================] - 74s 32ms/step - loss: 0.6427 - dice_coef: 0.3573\n",
      "Epoch 3/20\n",
      "2308/2308 [==============================] - 74s 32ms/step - loss: 0.5876 - dice_coef: 0.4124\n",
      "Epoch 4/20\n",
      "2308/2308 [==============================] - 74s 32ms/step - loss: 0.5506 - dice_coef: 0.4494\n",
      "Epoch 5/20\n",
      "2308/2308 [==============================] - 74s 32ms/step - loss: 0.5234 - dice_coef: 0.4766\n",
      "Epoch 6/20\n",
      "2308/2308 [==============================] - 74s 32ms/step - loss: 0.5035 - dice_coef: 0.4965\n",
      "Epoch 7/20\n",
      "2308/2308 [==============================] - 74s 32ms/step - loss: 0.4908 - dice_coef: 0.5092\n",
      "Epoch 8/20\n",
      "2308/2308 [==============================] - 74s 32ms/step - loss: 0.4771 - dice_coef: 0.5229\n",
      "Epoch 9/20\n",
      "2308/2308 [==============================] - 74s 32ms/step - loss: 0.4653 - dice_coef: 0.5347\n",
      "Epoch 10/20\n",
      "2308/2308 [==============================] - 74s 32ms/step - loss: 0.4576 - dice_coef: 0.5424\n",
      "Epoch 11/20\n",
      "2308/2308 [==============================] - 74s 32ms/step - loss: 0.4493 - dice_coef: 0.5507\n",
      "Epoch 12/20\n",
      "2308/2308 [==============================] - 74s 32ms/step - loss: 0.4413 - dice_coef: 0.5587\n",
      "Epoch 13/20\n",
      "2308/2308 [==============================] - 74s 32ms/step - loss: 0.4331 - dice_coef: 0.5669\n",
      "Epoch 14/20\n",
      "2308/2308 [==============================] - 74s 32ms/step - loss: 0.4275 - dice_coef: 0.5725\n",
      "Epoch 15/20\n",
      "2308/2308 [==============================] - 74s 32ms/step - loss: 0.4229 - dice_coef: 0.5771\n",
      "Epoch 16/20\n",
      "2308/2308 [==============================] - 74s 32ms/step - loss: 0.4164 - dice_coef: 0.5836\n",
      "Epoch 17/20\n",
      "2308/2308 [==============================] - 74s 32ms/step - loss: 0.4132 - dice_coef: 0.5868\n",
      "Epoch 18/20\n",
      "2308/2308 [==============================] - 74s 32ms/step - loss: 0.4090 - dice_coef: 0.5910\n",
      "Epoch 19/20\n",
      "2308/2308 [==============================] - 74s 32ms/step - loss: 0.4028 - dice_coef: 0.5972\n",
      "Epoch 20/20\n",
      "2308/2308 [==============================] - 74s 32ms/step - loss: 0.3972 - dice_coef: 0.6028\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "# from unet import UNet\n",
    "\n",
    "# ダイス係数を計算する関数\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true = K.flatten(y_true)\n",
    "    y_pred = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true * y_pred)\n",
    "    return 2.0 * intersection / (K.sum(y_true) + K.sum(y_pred) + 1)\n",
    "\n",
    "\n",
    "# ロス関数\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1.0 - dice_coef(y_true, y_pred)\n",
    "\n",
    "\n",
    "# U-Netのトレーニングを実行する関数\n",
    "def train_unet():\n",
    "    # trainingDataフォルダ配下にleft_imagesフォルダを置いている\n",
    "    X_train, file_names = load_X('../Preprocess/mhd/', 'train')\n",
    "    # trainingDataフォルダ配下にleft_groundTruthフォルダを置いている\n",
    "    Y_train = load_Y('../Preprocess/GroundTruth/')\n",
    "\n",
    "    # 入力はBGR3チャンネル\n",
    "    input_channel_count = 1\n",
    "    # 出力はグレースケール1チャンネル\n",
    "    output_channel_count = 1\n",
    "    # 一番初めのConvolutionフィルタ枚数は64\n",
    "    first_layer_filter_count = 64\n",
    "    # U-Netの生成\n",
    "    network = UNet(input_channel_count, output_channel_count, first_layer_filter_count)\n",
    "    model = network.get_model()\n",
    "    model.compile(loss=dice_coef_loss, optimizer=Adam(), metrics=[dice_coef])\n",
    "    \n",
    "    model.summary()\n",
    "\n",
    "    BATCH_SIZE = 12\n",
    "    # 20エポック回せば十分\n",
    "    NUM_EPOCH = 20\n",
    "    history = model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=NUM_EPOCH, verbose=1)\n",
    "    model.save_weights('unet_weights.hdf5')\n",
    "\n",
    "\n",
    "# 学習後のU-Netによる予測を行う関数\n",
    "def predict():\n",
    "    import cv2\n",
    "\n",
    "    # testDataフォルダ配下にleft_imagesフォルダを置いている\n",
    "    X_test, file_names = load_X('../Preprocess/mhd/', 'test')\n",
    "\n",
    "    input_channel_count = 1\n",
    "    output_channel_count = 1\n",
    "    first_layer_filter_count = 64\n",
    "    network = UNet(input_channel_count, output_channel_count, first_layer_filter_count)\n",
    "    model = network.get_model()\n",
    "    model.load_weights('unet_weights.hdf5')\n",
    "    BATCH_SIZE = 12\n",
    "    Y_pred = model.predict(X_test, BATCH_SIZE)\n",
    "    \n",
    "    # 変な方向の断面図でないか、気になる！\n",
    "    print(Y_pred.shape)\n",
    "\n",
    "    for i, y in enumerate(Y_pred):\n",
    "        # testDataフォルダ配下にleft_imagesフォルダを置いている\n",
    "        save_path = 'U-Net_DataSet/pred/'\n",
    "        if os.path.exists(save_path): os.mkdir(save_path)\n",
    "        #img = cv2.imread(save_path + file_names[i]+\".png\")\n",
    "        #y = cv2.resize(y, (img.shape[1], img.shape[0]))\n",
    "        cv2.imwrite(save_path+'prediction' + str(i) + '.png', denormalize_y(y))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_unet()\n",
    "#     predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF2-NT",
   "language": "python",
   "name": "tf2-nt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
